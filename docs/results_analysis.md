# Results Analysis

This document explains how to interpret and analyze the results generated by the UA-SLSM MLP Training Pipeline.

## Results Directory Structure

After running the pipeline, results are organized as follows:

```
results/
├── mlp_training_output/
│   ├── normalized/
│   │   ├── mobilenet_v3/
│   │   │   ├── fold_1/
│   │   │   │   ├── model.pth
│   │   │   │   ├── training_curves.png
│   │   │   │   ├── roc_curve.png
│   │   │   │   └── confusion_matrix_*.png
│   │   │   ├── ...
│   │   │   ├── combined_training_curves.png
│   │   │   ├── combined_confusion_matrix_*.png
│   │   │   ├── model_info.json
│   │   │   └── cross_validation_summary.json
│   │   └── ...
│   └── rgb/
│       └── ...
└── consolidated_results_table.tex  # LaTeX table with all results
```

## Individual Model Results

### Model Information

Each model directory contains a `model_info.json` file with basic information about the model:

```json
{
  "model_type": "resnet50",
  "data_version": "normalized",
  "num_classes": 2,
  "input_dim": 2048,
  "hidden_dims": [1024, 512],
  "dropout_rate": 0.6,
  "learning_rate": 5e-5,
  "weight_decay": 1e-4
}
```

### Cross-validation Summary

The `cross_validation_summary.json` file contains aggregated metrics across all folds:

```json
{
  "mean_f1": 0.923,
  "std_f1": 0.015,
  "mean_accuracy": 0.918,
  "std_accuracy": 0.012,
  "mean_auc": 0.967,
  "std_auc": 0.008,
  "param_count": 2834433,
  "avg_train_time_per_epoch": 0.45,
  "avg_inference_time_per_batch": 0.0023
}
```

### Training Curves

Each fold directory contains a `training_curves.png` file showing the training and validation loss/accuracy over epochs:

- **Loss Curves**: Show how the loss decreases over time
- **Accuracy Curves**: Show how the accuracy increases over time

The combined training curves (`combined_training_curves.png`) show the average performance across all folds.

### ROC Curves

Each fold directory contains an `roc_curve.png` file showing the Receiver Operating Characteristic curve:

- **X-axis**: False Positive Rate
- **Y-axis**: True Positive Rate
- **AUC**: Area Under the Curve (higher is better)

### Confusion Matrices

Each fold directory contains confusion matrix visualizations:

- **Raw Counts**: `confusion_matrix_counts.png`
- **Normalized**: `confusion_matrix_normalized.png`

The combined confusion matrices show the aggregated results across all folds.

## Consolidated Results

The `consolidated_results_table.tex` file contains a LaTeX table comparing all models and data versions:

### Performance Metrics

- **F1 Score**: Harmonic mean of precision and recall (higher is better)
- **Accuracy**: Overall correctness percentage (higher is better)
- **AUC**: Area Under ROC Curve (higher is better)

### Efficiency Metrics

- **Parameter Count**: Number of trainable parameters (lower is more efficient)
- **Training Time**: Average time per epoch in seconds (lower is faster)
- **Inference Time**: Average time per batch in milliseconds (lower is faster)

## Interpreting the Results

### Comparing Model Performance

When comparing models, consider:

1. **Performance Metrics**:
   - Higher F1, Accuracy, and AUC indicate better performance
   - Check standard deviations to assess consistency across folds

2. **Efficiency Metrics**:
   - Lower parameter count indicates a more compact model
   - Lower training and inference times indicate better computational efficiency

3. **Trade-offs**:
   - Larger models (e.g., ResNet50) may have better performance but slower inference
   - Smaller models (e.g., MobileNetV3) may be faster but less accurate

### Analyzing Confusion Matrices

Confusion matrices help identify specific classification issues:

- **True Positives (TP)**: Correctly predicted positive cases
- **False Positives (FP)**: Incorrectly predicted positive cases
- **True Negatives (TN)**: Correctly predicted negative cases
- **False Negatives (FN)**: Incorrectly predicted negative cases

Look for patterns in misclassifications to identify potential areas for improvement.

### Examining Training Curves

Training curves can reveal issues like:

- **Overfitting**: Validation loss increases while training loss continues to decrease
- **Underfitting**: Both training and validation loss remain high
- **Early Convergence**: Loss plateaus early, suggesting learning rate may be too high
- **Slow Convergence**: Loss decreases very slowly, suggesting learning rate may be too low

## Generating LaTeX Tables

The consolidated results table is generated automatically by the `consolidate_results.sh` script:

```bash
cd modules/mlp
./consolidate_results.sh
```

You can customize the table with options:

```bash
./consolidate_results.sh --no-highlight-best --no-efficiency
```

## Exporting Results

To export results for further analysis:

1. **LaTeX Table**: Use the generated `consolidated_results_table.tex` in your publications
2. **CSV Data**: Each model directory contains a `metrics.csv` file with detailed metrics
3. **JSON Data**: Use the `cross_validation_summary.json` files for programmatic analysis

## Best Practices

1. **Compare Similar Settings**: When comparing models, ensure they use the same data version and classification mode
2. **Check Statistical Significance**: Consider the standard deviations when comparing models
3. **Balance Performance and Efficiency**: Choose models based on your specific requirements
4. **Examine Failure Cases**: Use confusion matrices to understand where models struggle
5. **Consider Ensemble Methods**: If multiple models perform well in different areas, consider ensemble approaches
